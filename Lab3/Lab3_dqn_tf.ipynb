{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn8Nqr3HfiGa"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BeyKfDLnjf3",
        "outputId": "f5c288fa-266d-4cc7-fdd5-6eea365d0af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "_N8eIgvgfiGg",
        "outputId": "6c408774-b692-4ff0-d6d3-5ae6edc8e345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mieor4575-spring2022\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220331_214840-1u8u7dde</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ieor4575-spring2022/lab3/runs/1u8u7dde\" target=\"_blank\">eternal-wind-758</a></strong> to <a href=\"https://wandb.ai/ieor4575-spring2022/lab3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "run=wandb.init(project=\"lab3\", entity=\"ieor4575-spring2022\", tags=[\"tf2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zLnk0UxfiGh"
      },
      "source": [
        "## DQN (Deep Q Network)\n",
        "\n",
        "In previous Labs, we have learned to use Tensorflow to build deep learning models. In this lab, we will apply deep learning as function approximations in reinforcement learning. \n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon-$greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$ \n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter). \n",
        "\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nHyrtL1fiGj"
      },
      "source": [
        "### Bellman optimality equation\n",
        "\n",
        "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
        "\n",
        "$$Q^\\ast(s_t,a_t) = \\mathbb{E}\\big[r_t + \\gamma \\max_a Q^\\ast(s_{t+1},a)\\big]$$\n",
        "\n",
        "where the expectation is wrt the random reward $r_t$ and transition to the next state $s_{t+1}$. A natural objective to consider is \n",
        "\n",
        "$$\\min_\\theta\\  (Q_\\theta(s_t,a_t) -\\mathbb{E}\\big[r_t + \\gamma  \\max_a  Q_{\\hat \\theta}(s_{t+1},a)\\big])^2$$\n",
        "at the current or previous $\\hat \\theta$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDPTpWqCfiGk"
      },
      "source": [
        "### Building the DQN model\n",
        "\n",
        "The first step is to build a neural network with parameters $\\theta$ that predicts $Q_\\theta(s,a)$ for any $(s,a)$. You can either build a network that \n",
        "* takes as input a concatenated representation of state and action $(s,a)$ and output one dimensional score $Q_\\theta(s,a)$,\n",
        "\n",
        "or (in case of small number $K$ of discrete actions)\n",
        "\n",
        "* takes as input a  representation of state $s$ and outputs a $K$-dimensional vector giving scores $Q(s,a), a=1,\\ldots, K$ for all actions\n",
        "\n",
        "Below we have provided a skeleton code (incomplete) for defining and training the Q-function. **You need to fill in the DNN model definition and loss function definition**. Refer to regression lab (lab 2) for help."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wl3ZynJRfiGl"
      },
      "outputs": [],
      "source": [
        "# define neural net Q_\\theta(s,a) as a class\n",
        "\n",
        "class Qfunction(object):\n",
        "    \n",
        "    def __init__(self, obssize, actsize, lr):\n",
        "        \"\"\"\n",
        "        obssize: dimension of state space\n",
        "        actsize: dimension of action space\n",
        "        sess: sess to execute this Qfunction\n",
        "        optimizer: \n",
        "        \"\"\"\n",
        "        # DEFINE THE MODEL\n",
        "        self.model = tf.keras.models.Sequential([\n",
        "                                                 tf.keras.layers.Input(shape = (obssize,)),\n",
        "                                                 tf.keras.layers.Dense(64, activation = \"relu\", input_shape=(obssize,)),\n",
        "                                                 tf.keras.layers.Dense(64, activation = \"relu\"),\n",
        "                                                 tf.keras.layers.Dense(actsize, activation = \"linear\")\n",
        "                ])\n",
        "        \n",
        "        # DEFINE THE OPTIMIZER\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        \n",
        "        # RECORD HYPER-PARAMS\n",
        "        self.obssize = obssize\n",
        "        self.actsize = actsize\n",
        "        \n",
        "        # TEST\n",
        "        self.compute_Qvalues(np.random.randn(obssize).reshape(1, -1), 0)\n",
        "        self.compute_argmaxQ(np.random.randn(obssize).reshape(1, -1))\n",
        "    \n",
        "    def compute_Qvalues(self, states, actions):\n",
        "        \"\"\"\n",
        "        input: list of numsamples state-action pairs\n",
        "        output: List of Q values for each input (s,a). The output will have size [numsamples, 1] \n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #This will be different for neural network that takes as input a state-action pair\n",
        "        \n",
        "        q_preds = tf.cast(self.model(states), tf.double)\n",
        "        action_onehot = tf.cast(tf.one_hot(actions, self.actsize), tf.double)\n",
        "        q_preds_selected = tf.reduce_sum(q_preds * action_onehot, axis=-1)\n",
        "\n",
        "        return q_preds_selected\n",
        "        \n",
        "    def compute_maxQvalues(self, states):\n",
        "        \"\"\"\n",
        "        input: a list of numsamples states \n",
        "        output: max_a Q(s,a) values for every input state s in states. The output will have size numsamples\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "        \n",
        "        Qvalues = tf.cast(self.model(states), tf.double).numpy()\n",
        "        q_preds_greedy = np.max(Qvalues,1)\n",
        "        \n",
        "        return  q_preds_greedy\n",
        "        \n",
        "    def compute_argmaxQ(self, state):\n",
        "        \"\"\"\n",
        "        input: one state s\n",
        "        output: arg max_a Q(s,a) values for the input state s. The output will have size 1\n",
        "        \"\"\"\n",
        "        #Below is example code when neural network is set to take as input state and output Q-value for all actions.\n",
        "        #if the neural takes as input a state-action pair, then the code will need to loop over all actions to compute all values\n",
        "        \n",
        "        values = tf.cast(self.model(state), tf.double).numpy()\n",
        "        greedy_action = np.argmax(values.flatten())\n",
        "        \n",
        "        return greedy_action\n",
        "        \n",
        "        \n",
        "    def train(self, states, actions, targets):\n",
        "        \"\"\"\n",
        "        states: numpy array as input to compute loss (s)\n",
        "        actions: numpy array as input to compute loss (a)\n",
        "        targets: numpy array as input to compute loss (Q targets)\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "                        \n",
        "            # COMPUTE Q PREDICTIONS for each state-action pair \n",
        "            q_preds_selected = self.compute_Qvalues(states, actions)\n",
        "                      \n",
        "            # CONSTRUCT LOSS FUNCTION as mean square error between  predicted $Q$-value and targets for each sample \n",
        "            #print(q_preds_selected.shape, targets.shape)\n",
        "            loss = tf.reduce_mean((q_preds_selected - targets)**2)\n",
        "            \n",
        "            \n",
        "            # BACKWARD PASS\n",
        "            gradients = tape.gradient(loss, self.model.trainable_variables)  \n",
        "        \n",
        "            # UPDATE\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "            \n",
        "        return loss.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jYRhTFfiGn"
      },
      "source": [
        "At this point you can skip ahead to implementing the basic Q-learning that at every step $t$ in the environment\n",
        "* given state $s_t$, computes greedy actions from Q-values (using compute_argmaxQ function above) and uses $\\epsilon$-greedy select an action $a_t$, \n",
        "* makes observation of reward $r_t$ and next state $s_{t+1}$\n",
        "* using compute_maxQvalues() function, computes target\n",
        "  $$r_t + \\gamma \\max_a Q_\\theta(s_{t+1},a)$$\n",
        "and then retrains the Q-function using train() function above (with numsamples=1)\n",
        "\n",
        "However, for improved performance you may want to consider ideas like batch training (numsamples>1 is the batch size) with experience replay buffer and target-networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIzbkXCafiGo"
      },
      "source": [
        "**Replay Buffer**\n",
        "\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_i,a_i,r_i,s_{i}') \\sim R$ and then compute\n",
        "targets\n",
        "\n",
        "$$d_i=r_i + \\max_a \\gamma Q_{\\theta}(s_i^\\prime,a)$$\n",
        "for all $i$. Use the above training function train() with input as list $(s_i, a_i, d_i)_{i=1}^N$  to update parameters using backprop.\n",
        "\n",
        "**Target Network**\n",
        "\n",
        "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta^-$ is copied from the principal network every $\\tau$ time steps. Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i =  r_t + \\gamma \\max_a Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggBNFvx3fiGp"
      },
      "source": [
        "Hence several critical parts of the complete pseudocode for DQN is as follows:\n",
        "\n",
        "**Initialization.**\n",
        "principal network $Q_\\theta(s,a)$, target network $Q_{\\theta^{-}}(s,a)$. Replay buffer $R = \\{\\}$ (empty). \n",
        "\n",
        "**At each time step $t.$**\n",
        "The agent executes action using $\\epsilon-$greedy based on the principal network $Q_\\theta(s,a)$. To update $\\theta$: sample $N$ tuples $(s_i,a_i,r_i,s_i^\\prime) \\sim R$, compute empirical loss \n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - (r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameter $\\theta$ using backprop (just take one gradient step).\n",
        "\n",
        "**Update target network.**\n",
        "Every $\\tau$ time steps, update target network by copying $\\theta^{-} \\leftarrow \\theta$.\n",
        "\n",
        "**Bellman target.**\n",
        "Above, we have defined the target values as being computed from a target net with parameter $\\theta^-$ \n",
        "$$r_i + \\gamma \\max_a Q_{\\theta^{-}}(s_i^\\prime,a)$$\n",
        "It is worth thinking about what happens if we are at the end of an episode, that is, what if $s_i^\\prime$ here is a terminal state. In this case, should the Bellman target be defined exactly the same as above? Do we need some modifications? Think carefully about this as this will greatly impact the algorithmic performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Bc5bxUfiGp"
      },
      "source": [
        "### Implementation of replay buffer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBkQJWfqfiGq"
      },
      "outputs": [],
      "source": [
        "# Implement replay buffer\n",
        "import random\n",
        "class ReplayBuffer(object):\n",
        "    \n",
        "    def __init__(self, maxlength):\n",
        "        \"\"\"\n",
        "        maxlength: max number of tuples to store in the buffer\n",
        "        if there are more tuples than maxlength, pop out the oldest tuples\n",
        "        \"\"\"\n",
        "        self.buffer = deque()\n",
        "        self.number = 0\n",
        "        self.maxlength = maxlength\n",
        "    \n",
        "    def append(self, experience):\n",
        "        \"\"\"\n",
        "        this function implements appending new experience tuple\n",
        "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        self.buffer.append(experience)\n",
        "        self.number += 1\n",
        "        \n",
        "    def pop(self):\n",
        "        \"\"\"\n",
        "        pop out the oldest tuples if self.number > self.maxlength\n",
        "        \"\"\"\n",
        "        while self.number > self.maxlength:\n",
        "            self.buffer.popleft()\n",
        "            self.number -= 1\n",
        "    \n",
        "    def sample(self, batchsize):\n",
        "        \"\"\"\n",
        "        this function samples 'batchsize' experience tuples\n",
        "        batchsize: size of the minibatch to be sampled\n",
        "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
        "        \"\"\"\n",
        "        minibatch = random.sample(self.buffer,batchsize)\n",
        "        return minibatch\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb0uW4OafiGr"
      },
      "source": [
        "### Code snippet for copying target network\n",
        "You may use th following to update target network i.e. to copy from principal network to target network. We need to use tensorflow scope to distinguish the computational graphs of target and principal networks. The following function builds a tensorflow operation that does the copying $\\theta^- \\leftarrow \\theta$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3luDczbLfiGr"
      },
      "outputs": [],
      "source": [
        "def run_target_update(Qprincipal, Qtarget):\n",
        "    for v,v_ in zip(Qprincipal.model.trainable_variables, Qtarget.model.trainable_variables):\n",
        "        v_.assign(v.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQb1jbkAfiGs"
      },
      "source": [
        "## Main code for DQN\n",
        "Now that we have all the ingredients for DQN, we can write the main procedure to train DQN on a given environment. The implementation is straightforward if you follow the pseudocode pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF8xpIozfiGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17271a39-abf6-4af4-d98f-35cfd2eecf2d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7fc1b1824190>"
            ],
            "text/html": [
              "<iframe src=\"https://wandb.ai/ieor4575-spring2022/lab3/runs/1u8u7dde?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "buffersize 61\n",
            "iteration 0 ave training returns 61.0\n",
            "buffersize 373\n",
            "iteration 10 ave training returns 31.2\n",
            "buffersize 695\n",
            "iteration 20 ave training returns 32.2\n",
            "buffersize 987\n",
            "iteration 30 ave training returns 29.2\n",
            "buffersize 1181\n",
            "iteration 40 ave training returns 19.4\n",
            "buffersize 1382\n",
            "iteration 50 ave training returns 20.1\n",
            "buffersize 1948\n",
            "iteration 60 ave training returns 56.6\n",
            "buffersize 2000\n",
            "iteration 70 ave training returns 133.8\n",
            "buffersize 2000\n",
            "iteration 80 ave training returns 145.7\n",
            "buffersize 2000\n",
            "iteration 90 ave training returns 165.3\n",
            "buffersize 2000\n",
            "iteration 100 ave training returns 154.2\n",
            "buffersize 2000\n",
            "iteration 110 ave training returns 179.6\n",
            "buffersize 2000\n",
            "iteration 120 ave training returns 172.6\n",
            "buffersize 2000\n",
            "iteration 130 ave training returns 200.0\n",
            "buffersize 2000\n",
            "iteration 140 ave training returns 185.2\n",
            "buffersize 2000\n",
            "iteration 150 ave training returns 190.1\n",
            "buffersize 2000\n",
            "iteration 160 ave training returns 193.9\n",
            "buffersize 2000\n",
            "iteration 170 ave training returns 199.8\n",
            "buffersize 2000\n",
            "iteration 180 ave training returns 200.0\n",
            "buffersize 2000\n",
            "iteration 190 ave training returns 195.5\n",
            "buffersize 2000\n",
            "iteration 200 ave training returns 170.6\n",
            "buffersize 2000\n",
            "iteration 210 ave training returns 200.0\n",
            "buffersize 2000\n",
            "iteration 220 ave training returns 200.0\n",
            "buffersize 2000\n",
            "iteration 230 ave training returns 200.0\n",
            "buffersize 2000\n",
            "iteration 240 ave training returns 200.0\n"
          ]
        }
      ],
      "source": [
        "%%wandb\n",
        "#remove above line if you do not want to see inline plots from wandb\n",
        "\n",
        "# hyper-parameters\n",
        "lr = 1e-3  # learning rate for gradient update\n",
        "batchsize = 64  # batchsize for buffer sampling\n",
        "maxlength = 2000  # max number of tuples held by buffer\n",
        "envname = \"CartPole-v0\"  # environment name\n",
        "tau = 100  # time steps for target update\n",
        "episodes = 250  # number of episodes to run\n",
        "initialsize = 1000  # initial time steps before start training\n",
        "gamma = .99  # discount\n",
        "epsilon_decay = 0.99\n",
        "epsilon = .8\n",
        "\n",
        "USING_BUFFER = True\n",
        "# initialize environment\n",
        "env = gym.make(envname)\n",
        "obssize = env.observation_space.low.size\n",
        "actsize = env.action_space.n\n",
        "\n",
        "# initialize Q-function networks (princpal and target)\n",
        "Qprincipal = Qfunction(obssize, actsize, lr)\n",
        "Qtarget = Qfunction(obssize, actsize, lr)\n",
        "\n",
        "# initialization of graph and buffer\n",
        "buffer = ReplayBuffer(maxlength)\n",
        "\n",
        "# main iteration\n",
        "rrecord = []\n",
        "totalstep = 0\n",
        "for episode in range(episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    rsum = 0\n",
        "    epsilon *= epsilon_decay\n",
        "    while not done:\n",
        "        #greedy choice below. Use epsilon greedy for exploration\n",
        "        u = tf.random.uniform(shape = [], dtype = tf.float32)\n",
        "        if u> epsilon :\n",
        "          action = Qprincipal.compute_argmaxQ(np.expand_dims(obs,0))\n",
        "        else :\n",
        "          action = env.action_space.sample()\n",
        "        \n",
        "        newobs, r, done, _ = env.step(action)\n",
        "        done_ = 0 if done else 1\n",
        "        e = {\"s\" : obs, \"a\" : action, \"r\" : r, \"d\" : done_, \"s'\": newobs}\n",
        "\n",
        "        #IF NOT USING BUFFER:\n",
        "        #use single sample (obs, action, r, done_, newobs) with Qtarget to compute target and train Qprincipal\n",
        "        if not USING_BUFFER:\n",
        "          # If we have reached a final state s', we take Q(s',.) = 0\n",
        "          if done :\n",
        "            target = r\n",
        "          else :\n",
        "            target = r + gamma*Qprincipal.compute_maxQvalues(tf.reshape(newobs,(1,obssize)))\n",
        "          Qprincipal.train(tf.reshape(obs,(1,obssize)), action, target)\n",
        "        \n",
        "        \n",
        "        # ELSE IF USING REPLAY BUFFER\n",
        "        if USING_BUFFER :\n",
        "          # append experiences e to buffer\n",
        "          buffer.append(e)\n",
        "          while buffer.number > maxlength:\n",
        "              buffer.pop()\n",
        "          #every few episodes (decide the frequency) sample a minibatch from buffer\n",
        "          if totalstep>initialsize :\n",
        "            batch = buffer.sample(batchsize=batchsize)\n",
        "            #compute targets in batch using Qtarget\n",
        "            targets = np.array([e[\"r\"] for e in batch])+Qtarget.compute_maxQvalues(np.array([e[\"s'\"] for e in batch]))*np.array([e[\"d\"] for e in batch])\n",
        "            #and train  Qprincipal\n",
        "            Qprincipal.train(np.array([e[\"s\"] for e in batch]), np.array([e[\"a\"] for e in batch]), targets)\n",
        "        \n",
        "            #UPDATE target network \n",
        "            #every tau steps update copy the principal network to the target network\n",
        "            if totalstep % tau == 0:\n",
        "                run_target_update(Qprincipal, Qtarget)\n",
        "        # update\n",
        "        totalstep += 1\n",
        "        rsum += r\n",
        "        obs = newobs \n",
        "        \n",
        "    #Below are for printing and debugging at the end of episode\n",
        "    rrecord.append(rsum)\n",
        "    if episode % 10 == 0:\n",
        "        # Feel free to add printing functions for debugging purposes\n",
        "        print('buffersize {}'.format(buffer.number))\n",
        "        print('iteration {} ave training returns {}'.format(episode, np.mean(rrecord[-10:])))\n",
        "        \n",
        "    #printing moving averages for smoothed visualization. \n",
        "    fixedWindow=100\n",
        "    movingAverage=0\n",
        "    if len(rrecord) >= fixedWindow:\n",
        "        movingAverage=np.mean(rrecord[len(rrecord)-fixedWindow:len(rrecord)-1])\n",
        "    wandb.log({ \"training reward\" : rsum, \"train reward moving average\" : movingAverage})\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMR-ALm7fiGt"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, we evaluate the performance of the trained agent. We will evaluate the performance of the greedy policy wrt learned Q-function. The evaluation will be run 10 times, each for eval_epsiodes and print out the average performance across these episodes. Please do not change the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Oq3JCBfiGt"
      },
      "outputs": [],
      "source": [
        "### DO NOT CHANGE\n",
        "def evaluate(Q, env, episodes):\n",
        "    # main iteration\n",
        "    score = 0.0\n",
        "    for episode in range(episodes):\n",
        "        \n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        rsum = 0\n",
        "        \n",
        "        while not done:\n",
        "            # always greedy\n",
        "            action = Q.compute_argmaxQ(np.expand_dims(obs,0))\n",
        "            \n",
        "\n",
        "            # mdp stepping forward\n",
        "            newobs, r, done, _ = env.step(action)\n",
        "\n",
        "            # update data\n",
        "            rsum += r\n",
        "            obs = newobs        \n",
        "\n",
        "        \n",
        "        wandb.log({\"eval reward\" : rsum})\n",
        "        score = score + rsum\n",
        "    score = score/episodes\n",
        "    \n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaOcd85kfiGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4241ee-8418-4a64-e743-c5d27dd8f5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval performance of DQN agent: 200.0\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE CODE HERE\n",
        "# after training, we will evaluate the performance of the agent\n",
        "# on a target environment\n",
        "env_test = gym.make(envname)\n",
        "eval_episodes = 1000\n",
        "score = evaluate(Qprincipal, env_test, eval_episodes)\n",
        "wandb.run.summary[\"score\"]=score \n",
        "\n",
        "print(\"eval performance of DQN agent: {}\".format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWx4WFkffiGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287,
          "referenced_widgets": [
            "219968bf0a624e5ea181e53723bdccf8",
            "3435a7e8db9643c5985165e1b12937ef",
            "24818af230e54ca5ac9bff03d2dcacf1",
            "2d48a6bb2e27460cb283b9b20c61b07e",
            "b657969620f34e1f8c68ebfe0d1909d6",
            "57229ae6a6d842739df6274e55faa346",
            "b3d7176e03ab459abf8ee0663d5f26f9",
            "a902015f1a074e638486adcb6af77b4b"
          ]
        },
        "outputId": "85106a09-85cf-4a97-c5a6-1b51280ef6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "219968bf0a624e5ea181e53723bdccf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train reward moving average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▅▅▅▆▆▆▇▇▇▇████████████</td></tr><tr><td>training reward</td><td>▁▁▂▂▃▃▁▂▂▅▅██▃█▇███████████████▄████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval reward</td><td>200.0</td></tr><tr><td>score</td><td>200.0</td></tr><tr><td>train reward moving average</td><td>195.63636</td></tr><tr><td>training reward</td><td>200.0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">eternal-wind-758</strong>: <a href=\"https://wandb.ai/ieor4575-spring2022/lab3/runs/1u8u7dde\" target=\"_blank\">https://wandb.ai/ieor4575-spring2022/lab3/runs/1u8u7dde</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220331_214840-1u8u7dde/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copie de lab3_dqn_tf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "219968bf0a624e5ea181e53723bdccf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3435a7e8db9643c5985165e1b12937ef",
              "IPY_MODEL_24818af230e54ca5ac9bff03d2dcacf1"
            ],
            "layout": "IPY_MODEL_2d48a6bb2e27460cb283b9b20c61b07e"
          }
        },
        "3435a7e8db9643c5985165e1b12937ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b657969620f34e1f8c68ebfe0d1909d6",
            "placeholder": "​",
            "style": "IPY_MODEL_57229ae6a6d842739df6274e55faa346",
            "value": "0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "24818af230e54ca5ac9bff03d2dcacf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d7176e03ab459abf8ee0663d5f26f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a902015f1a074e638486adcb6af77b4b",
            "value": 1
          }
        },
        "2d48a6bb2e27460cb283b9b20c61b07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b657969620f34e1f8c68ebfe0d1909d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57229ae6a6d842739df6274e55faa346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3d7176e03ab459abf8ee0663d5f26f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a902015f1a074e638486adcb6af77b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}